{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1a6604a",
   "metadata": {},
   "source": [
    "# Using the SageMaker SDK with built-in algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23c337d",
   "metadata": {},
   "source": [
    "\n",
    "***Being familiar with the SageMaker SDK is important to making the most of SageMaker.\n",
    "You can find its documentation at https://sagemaker.readthedocs.io .\n",
    "Walking through a simple example is the best way to get started. In this section, we'll use\n",
    "the Linear Learner algorithm to train a regression model on the Boston Housing dataset.\n",
    "We'll proceed very slowly, leaving no stone unturned. Once again, these concepts are\n",
    "essential***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d0e9ea",
   "metadata": {},
   "source": [
    "## Preparing data\n",
    "\n",
    "***Built-in algorithms expect the dataset to be in a certain format, such as CSV, protobuf, or libsvm. Supported formats are listed in the algorithm documentation. For instance, Linear Learner supports CSV and recordIO-wrapped protobuf ( https://docs.aws.amazon.com/sagemaker/latest/dg/linear-learner.html#ll-input_output ). Our input dataset is already in the repository in CSV format, so let's use that. Dataset preparation will be extremely simple, and we'll run it manually: ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5a548b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  archive.zip\n",
      "  inflating: Boston.csv              \n"
     ]
    }
   ],
   "source": [
    "!unzip archive.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "27bb10f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Using pandas , we load the CSV dataset with pandas:\n",
    "\n",
    "import pandas as pd\n",
    "dataset = pd.read_csv('housing.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "794b0efc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(506, 15)\n"
     ]
    }
   ],
   "source": [
    "# 2. Then, we print the shape of the dataset:\n",
    "\n",
    "print(dataset.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "97b9b2d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>crim</th>\n",
       "      <th>zn</th>\n",
       "      <th>indus</th>\n",
       "      <th>chas</th>\n",
       "      <th>nox</th>\n",
       "      <th>rm</th>\n",
       "      <th>age</th>\n",
       "      <th>dis</th>\n",
       "      <th>rad</th>\n",
       "      <th>tax</th>\n",
       "      <th>ptratio</th>\n",
       "      <th>black</th>\n",
       "      <th>lstat</th>\n",
       "      <th>medv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1</td>\n",
       "      <td>296</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0     crim    zn  indus  chas    nox     rm   age     dis  rad  \\\n",
       "0           1  0.00632  18.0   2.31     0  0.538  6.575  65.2  4.0900    1   \n",
       "1           2  0.02731   0.0   7.07     0  0.469  6.421  78.9  4.9671    2   \n",
       "2           3  0.02729   0.0   7.07     0  0.469  7.185  61.1  4.9671    2   \n",
       "3           4  0.03237   0.0   2.18     0  0.458  6.998  45.8  6.0622    3   \n",
       "4           5  0.06905   0.0   2.18     0  0.458  7.147  54.2  6.0622    3   \n",
       "\n",
       "   tax  ptratio   black  lstat  medv  \n",
       "0  296     15.3  396.90   4.98  24.0  \n",
       "1  242     17.8  396.90   9.14  21.6  \n",
       "2  242     17.8  392.83   4.03  34.7  \n",
       "3  222     18.7  394.63   2.94  33.4  \n",
       "4  222     18.7  396.90   5.33  36.2  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. Now, we display the first 5 lines of the dataset:\n",
    "\n",
    "dataset[:5]\n",
    "\n",
    "# This prints out the table visible in the following diagram. For each house, we see\n",
    "# 12 features, and a target attribute ( medv ) set to the median value of the house in\n",
    "# thousands of dollars:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3dd93e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Reading the algorithm documentation ( https://docs.aws.amazon.com/\n",
    "# sagemaker/latest/dg/cdf-training.html ), we see that Amazon\n",
    "# SageMaker requires that a CSV file doesn't have a header record and that the target\n",
    "# variable is in the first column. Accordingly, we move the medv column to the front\n",
    "# of the dataframe:\n",
    "dataset = pd.concat([dataset['medv'],dataset.drop(['medv'], axis=1)],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "45d32357",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. A bit of scikit-learn magic helps split the dataframe up into two parts: 90% for training, and 10% for validation:\n",
    "from sklearn.model_selection import train_test_split\n",
    "training_dataset, validation_dataset = train_test_split(dataset, test_size=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e4496f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. We save these two splits to individual CSV files, without either an index or a header:\n",
    "training_dataset.to_csv('training_dataset.csv',index=False, header=False)\n",
    "validation_dataset.to_csv('validation_dataset.csv',index=False, header=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c01f375c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. We now need to upload these two files to S3. We could use any bucket, and\n",
    "# here we'll use the default bucket conveniently created by SageMaker in the region\n",
    "# we're running in. We can find its name with the sagemaker.Session.\n",
    "# default_bucket() API:\n",
    "import sagemaker\n",
    "sess = sagemaker.Session()\n",
    "bucket = sess.default_bucket()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "556b03de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-1-562547773519/boston-housing/input/training/training_dataset.csv\n",
      "s3://sagemaker-us-east-1-562547773519/boston-housing/input/validation/validation_dataset.csv\n"
     ]
    }
   ],
   "source": [
    "# 8. Finally, we use the sagemaker.Session.upload_data() API to upload the\n",
    "# two CSV files to the default bucket. Here, the training and validation datasets are\n",
    "# made of a single file each, but we could upload multiple files if needed. For this\n",
    "# reason, we must upload the datasets under different S3 prefixes, so that their files\n",
    "# won't be mixed up:\n",
    "prefix = 'boston-housing'\n",
    "training_data_path = sess.upload_data(path='training_dataset.csv',key_prefix=prefix + '/input/training')\n",
    "validation_data_path = sess.upload_data(path='validation_dataset.csv',key_prefix=prefix + '/input/validation')\n",
    "\n",
    "print(training_data_path)\n",
    "print(validation_data_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b236d6ee",
   "metadata": {},
   "source": [
    "#### Configuring a training job"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61dadf6",
   "metadata": {},
   "source": [
    "1. I hope we know that SageMaker algorithms are packaged in\n",
    "Docker containers. Using boto3 and the image_uris.retrieve() API, we\n",
    "can easily find the name of the Linear Learner algorithm in the region\n",
    "we're running:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d316e5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from sagemaker import image_uris\n",
    "region = boto3.Session().region_name\n",
    "container = image_uris.retrieve('linear-learner', region)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85754a0",
   "metadata": {},
   "source": [
    "2. Now that we know the name of the container, we can configure our training job\n",
    "with the Estimator object. In addition to the container name, we also pass the\n",
    "IAM role that SageMaker instances will use, the instance type and instance count\n",
    "to use for training, as well as the output location for the model. Estimator will\n",
    "generate a training job automatically, and we could also set our own prefix with the\n",
    "base_job_name parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ceec6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sagemaker.estimator import Estimator\n",
    "ll_estimator = Estimator(container,role=sagemaker.get_execution_role(),\n",
    "                         instance_count=1,\n",
    "                         instance_type='ml.m5.large',\n",
    "                         output_path='s3://{}/{}/output'.format(bucket,prefix))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b116e1",
   "metadata": {},
   "source": [
    "SageMaker supports plenty of different instance types, with some differences across\n",
    "AWS regions. You can find the full list at https://docs.aws.amazon.com/sagemaker/latest/dg/instance-types-az.html \n",
    "Which one should we use here? Looking at the Linear Learner documentation\n",
    "( https://docs.aws.amazon.com/sagemaker/latest/dg/linear-learner.html#ll-instances ), we see that you can train the Linear Learner\n",
    "algorithm on single- or multi-machine CPU and GPU instances. Here, we're working\n",
    "with a tiny dataset, so let's select the smallest training instance available in our\n",
    "region: ml.m5.large .\n",
    "Checking the pricing page ( https://aws.amazon.com/sagemaker/\n",
    "pricing/ ), we see that this instance costs $0.15 per hour in the eu-west-1 region\n",
    "(the one I'm using for this job).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af12c2ae",
   "metadata": {},
   "source": [
    "3. Next, we have to set hyperparameters. This step is possibly one of the most obscure\n",
    "and most difficult parts of any machine learning project. Here's my tried and\n",
    "tested advice: read the algorithm documentation, stick to mandatory parameters\n",
    "only unless you really know what you're doing, and quickly check optional\n",
    "parameters for default values that could clash with your dataset. \n",
    "Let's look at the documentation, and see which hyperparameters are mandatory\n",
    "( https://docs.aws.amazon.com/sagemaker/latest/dg/ll_hyperparameters.html) . As it turns out, there is only one: predictor_\n",
    "type . It defines the type of problem that Linear Learner is training on (regression,\n",
    "binary classification, or multiclass classification).\n",
    "Taking a deeper look, we see that the default value for mini_batch_size is 1000:\n",
    "this isn't going to work well with our 506-sample dataset, so let's set it to 32. We also\n",
    "learn that the normalize_data parameter is set to true by default, which makes\n",
    "it unnecessary to normalize data ourselves:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a05143",
   "metadata": {},
   "outputs": [],
   "source": [
    "ll_estimator.set_hyperparameters(predictor_type='regressor',mini_batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb03dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"4. Now, let's define the data channels: a channel is a named source of data passed to\n",
    "a SageMaker estimator. All built-in algorithms need at least a train channel, and\n",
    "many also accept additional channels for validation and testing. Here, we have two\n",
    "channels, which both provide data in CSV format. The TrainingInput() API\n",
    "lets us define their location, their format, whether they are compressed, and so on:\"\"\"\n",
    "\n",
    "training_data_channel = sagemaker.TrainingInput(s3_data=training_data_path,\n",
    "                                                content_type='text/csv')\n",
    "\n",
    "validation_data_channel = sagemaker.TrainingInput(s3_data=validation_data_path,\n",
    "                                                  content_type='text/csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c919f46b",
   "metadata": {},
   "source": [
    "\n",
    "**By default, data served by a channel will be fully copied to each training instance,\n",
    "which is fine for small datasets. We'll study alternatives in Chapter 10, Advanced\n",
    "Training Techniques.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef63222e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b7577d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c60aea8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
